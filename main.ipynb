{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer for time series modeling\n",
    "These are the inputs and outputs to the model.\n",
    "- Inputs: SWIT.\n",
    "- Output: Streamflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import utils\n",
    "import dataset as ds\n",
    "import transformer as tst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define needed functions: train, test, infer, nse, and parameters printer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define the training step\n",
    "def train(dataloader, model, src_mask, tgt_mask, loss_function, optimizer, device, df_training, epoch):\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    training_loss = [] # For plotting purposes\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        src, tgt, tgt_y = batch\n",
    "        src, tgt, tgt_y = src.to(device), tgt.to(device), tgt_y.to(device)\n",
    "\n",
    "        # Zero out gradients for every batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute prediction error\n",
    "        pred, sa_weights, mha_weights = model(src=src, tgt=tgt, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        pred = pred.to(device)\n",
    "        loss = loss_function(pred, tgt_y.unsqueeze(2))\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Save results for plotting\n",
    "        training_loss.append(loss.item())\n",
    "        epoch_train_loss = np.mean(training_loss)\n",
    "        df_training.loc[epoch] = [epoch, epoch_train_loss]\n",
    "\n",
    "        # if i % 20 == 0:\n",
    "        #     print('Current batch', i)\n",
    "        #     loss, current = loss.item(), (i + 1) * len(src)\n",
    "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "# Define testing step\n",
    "def val(dataloader, model, src_mask, tgt_mask, loss_function, device, df_validation, epoch):\n",
    "    \n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    validation_loss = [] # For plotting purposes\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            src, tgt, tgt_y = batch\n",
    "            src, tgt, tgt_y = src.to(device), tgt.to(device), tgt_y.to(device)\n",
    "            \n",
    "            pred, sa_weights, mha_weights = model(src=src, tgt=tgt, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "            pred = pred.to(device)\n",
    "            loss = loss_function(pred, tgt_y.unsqueeze(2))\n",
    "            \n",
    "            # Save results for plotting\n",
    "            validation_loss.append(loss.item())\n",
    "            epoch_val_loss = np.mean(validation_loss)\n",
    "            df_validation.loc[epoch] = [epoch, epoch_val_loss]\n",
    "    \n",
    "    loss /= num_batches\n",
    "    # print(f\"Avg test loss: {loss:>8f}\")\n",
    "\n",
    "# Define inference step\n",
    "def test(dataloader, model, src_mask, tgt_mask, device):\n",
    "    \n",
    "    # Get ground truth\n",
    "    tgt_y_truth = torch.zeros(len(dataloader))\n",
    "    for i, (src, tgt, tgt_y) in enumerate(dataloader):\n",
    "        tgt_y_truth[i] = tgt_y\n",
    "\n",
    "    # Define tensor to store the predictions\n",
    "    tgt_y_hat = torch.zeros((len(dataloader)), device=device)\n",
    "\n",
    "    # Define list to store the multi-head self attention weights\n",
    "    all_sa_weights_inference = []\n",
    "    all_mha_weights_inference = []\n",
    "    \n",
    "    # Perform inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, sample in enumerate(dataloader):\n",
    "            src, tgt, tgt_y = sample\n",
    "            src, tgt, tgt_y = src.to(device), tgt.to(device), tgt_y.to(device)\n",
    "\n",
    "            pred, sa_weights, mha_weights = model(src=src, tgt=tgt, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "            all_sa_weights_inference.append(sa_weights)\n",
    "            all_mha_weights_inference.append(mha_weights)\n",
    "            pred = pred.to(device)\n",
    "\n",
    "            tgt_y_hat[i] = pred\n",
    "\n",
    "    # Save inference attention for the last step\n",
    "    # np.save('all_sa_weights.npy', [sa_weight.cpu() for sa_weight in all_sa_weights_inference], allow_pickle=False, fix_imports=False)\n",
    "    # np.save('all_mha_weights.npy', [mha_weight.cpu() for mha_weight in all_mha_weights_inference], allow_pickle=False, fix_imports=False)\n",
    "    \n",
    "    # Pass target_y_hat to cpu for plotting purposes\n",
    "    tgt_y_hat = tgt_y_hat.cpu()\n",
    "\n",
    "    tgt_y_truth, tgt_y_hat = tgt_y_truth.numpy(), tgt_y_hat.numpy()\n",
    "\n",
    "    # Save ground truth and predictions\n",
    "    # np.save('tgt_y_truth.npy', tgt_y_truth, allow_pickle=False, fix_imports=False)\n",
    "    # np.save('tgt_y_hat.npy', tgt_y_hat, allow_pickle=False, fix_imports=False)\n",
    "    \n",
    "    return tgt_y_truth, tgt_y_hat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Hyperparams\n",
    "batch_size = 128\n",
    "validation_size = 0.125\n",
    "src_variables = ['X']\n",
    "tgt_variables = ['y']\n",
    "input_variables = src_variables + tgt_variables\n",
    "timestamp_col_name = \"time\"\n",
    "\n",
    "# Only use data from this date and onwards\n",
    "cutoff_date = datetime.datetime(1980, 1, 1) \n",
    "\n",
    "d_model = 32\n",
    "n_heads = 2\n",
    "n_decoder_layers = 0 # Remember that with the current implementation it always has a decoder layer that returns the weights\n",
    "n_encoder_layers = 1\n",
    "encoder_sequence_len = 1461 # length of input given to encoder used to create the pre-summarized windows (4 years of data) 1461\n",
    "crushed_encoder_sequence_len = 53 # Encoder sequence length afther summarizing the data when defining the dataset 53\n",
    "decoder_sequence_len = 1 # length of input given to decoder\n",
    "output_sequence_length = 1 # target sequence length. If hourly data and length = 48, you predict 2 days ahead\n",
    "window_size = encoder_sequence_len + output_sequence_length # used to slice data into sub-sequences\n",
    "step_size = 1 # Step size, i.e. how many time steps does the moving window move at each step\n",
    "in_features_encoder_linear_layer = 32\n",
    "in_features_decoder_linear_layer = 32\n",
    "max_sequence_len = encoder_sequence_len\n",
    "batch_first = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "device = ('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "data = utils.read_data(timestamp_col_name=timestamp_col_name)\n",
    "\n",
    "# Extract train and test data\n",
    "training_val_lower_bound = datetime.datetime(1980, 10, 1)\n",
    "training_val_upper_bound = datetime.datetime(2010, 9, 30)\n",
    "\n",
    "# Extract train/validation and test data\n",
    "training_val_data = data[(training_val_lower_bound <= data.index) & (data.index <= training_val_upper_bound)]\n",
    "testing_data = data[data.index > training_val_upper_bound]\n",
    "\n",
    "# Normalize the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit scaler on the training set\n",
    "scaler.fit(training_val_data.iloc[:, 1:])\n",
    "\n",
    "# Transform the training and test sets\n",
    "training_val_data.iloc[:, 1:] = scaler.transform(training_val_data.iloc[:, 1:])\n",
    "testing_data.iloc[:, 1:] = scaler.transform(testing_data.iloc[:, 1:])\n",
    "\n",
    "# Make list of (start_idx, end_idx) pairs that are used to slice the time series sequence into chuncks\n",
    "training_val_indices = utils.get_indices(data=training_val_data, window_size=window_size, step_size=step_size)\n",
    "\n",
    "# Divide train data into train and validation data with a 8:1 ratio using the indices.\n",
    "# This is done this way because we need 4 years of data to create the summarized nonuniform timesteps,\n",
    "# what limits the size of the validation set. However, with this implementation, we use data from the\n",
    "# traning part to build the summarized nonuniform timesteps for the validation set. For example, if\n",
    "# we use the current validation size, the set would have less than four years of data and would not\n",
    "# be able to create the summarized nonuniform timesteps.\n",
    "validation_size = 0.125\n",
    "training_indices = training_val_indices[:-(round(len(training_val_indices)*validation_size))]\n",
    "validation_indices = training_val_indices[(round(len(training_val_indices)*(1-validation_size))):]\n",
    "\n",
    "testing_indices = utils.get_indices(data=testing_data, window_size=window_size, step_size=step_size)\n",
    "\n",
    "# Make instance of the custom dataset class\n",
    "training_data = ds.TransformerDataset(data=torch.tensor(training_val_data[input_variables].values).float(),\n",
    "                                    indices=training_indices, encoder_sequence_len=encoder_sequence_len, \n",
    "                                    decoder_sequence_len=decoder_sequence_len, tgt_sequence_len=output_sequence_length)\n",
    "validation_data = ds.TransformerDataset(data=torch.tensor(training_val_data[input_variables].values).float(),\n",
    "                                    indices=validation_indices, encoder_sequence_len=encoder_sequence_len, \n",
    "                                    decoder_sequence_len=decoder_sequence_len, tgt_sequence_len=output_sequence_length)\n",
    "testing_data = ds.TransformerDataset(data=torch.tensor(testing_data[input_variables].values).float(),\n",
    "                                    indices=testing_indices, encoder_sequence_len=encoder_sequence_len, \n",
    "                                    decoder_sequence_len=decoder_sequence_len, tgt_sequence_len=output_sequence_length)\n",
    "\n",
    "# Set up dataloaders\n",
    "training_val_data = training_data + validation_data # For testing puporses\n",
    "training_data = DataLoader(training_data, batch_size, shuffle=True)\n",
    "validation_data = DataLoader(validation_data, shuffle=True)\n",
    "testing_data = DataLoader(testing_data, batch_size=1)\n",
    "training_val_data = DataLoader(training_val_data, batch_size=1) # For testing puporses\n",
    "\n",
    "# Update the encoder sequence length to its crushed version\n",
    "encoder_sequence_len = crushed_encoder_sequence_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the transformer model and send it to device\n",
    "model = tst.TimeSeriesTransformer(input_size=len(src_variables), decoder_sequence_len=decoder_sequence_len, \n",
    "                                batch_first=batch_first, d_model=d_model, n_encoder_layers=n_encoder_layers, \n",
    "                                n_decoder_layers=n_decoder_layers, n_heads=n_heads, dropout_encoder=0.2, \n",
    "                                dropout_decoder=0, dropout_pos_encoder=0.1, dim_feedforward_encoder=in_features_encoder_linear_layer, \n",
    "                                dim_feedforward_decoder=in_features_decoder_linear_layer, num_predicted_features=len(tgt_variables)).to(device)\n",
    "# Send model to device\n",
    "model.to(device)\n",
    "\n",
    "# Print model and number of parameters\n",
    "print('Defined model:\\n', model)\n",
    "utils.count_parameters(model)\n",
    "\n",
    "# Make src mask for the decoder with size\n",
    "# [batch_size*n_heads, output_sequence_length, encoder_sequence_len]\n",
    "src_mask = utils.unmasker(dim1=output_sequence_length, dim2=encoder_sequence_len).to(device)\n",
    "\n",
    "# Make tgt mask for decoder with size\n",
    "# [batch_size*n_heads, output_sequence_length, output_sequence_length]\n",
    "tgt_mask = utils.masker(dim1=output_sequence_length, dim2=output_sequence_length).to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Update model in the training process and test it\n",
    "epochs = 5 # 250\n",
    "start_time = time.time()\n",
    "df_training = pd.DataFrame(columns=('epoch', 'loss_train'))\n",
    "df_validation = pd.DataFrame(columns=('epoch', 'loss_test'))\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(training_data, model, src_mask, tgt_mask, loss_function, optimizer, device, df_training, epoch=t)\n",
    "    val(validation_data, model, src_mask, tgt_mask, loss_function, device, df_validation, epoch=t)\n",
    "print(\"Done! ---Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Inference\n",
    "tgt_y_truth_train_val, tgt_y_hat_train_val = test(training_val_data, model, src_mask, tgt_mask, device)\n",
    "tgt_y_truth_test, tgt_y_hat_test = test(testing_data, model, src_mask, tgt_mask, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "plt.figure(1);plt.clf()\n",
    "plt.plot(df_training['epoch'], df_training['loss_train'], '-o', label='loss train')\n",
    "plt.plot(df_training['epoch'], df_validation['loss_test'], '-o', label='loss test')\n",
    "plt.yscale('log')\n",
    "plt.xlabel(r'epoch')\n",
    "plt.ylabel(r'loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot testing results\n",
    "plt.figure(2);plt.clf()\n",
    "plt.plot(tgt_y_truth_train_val, label='observed')\n",
    "plt.plot(tgt_y_hat_train_val, label='predicted')\n",
    "plt.title('Training and validation results')\n",
    "plt.xlabel(r'time (days)')\n",
    "plt.ylabel(r'y')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(2);plt.clf()\n",
    "plt.plot(tgt_y_truth_test, label='observed')\n",
    "plt.plot(tgt_y_hat_test, label='predicted')\n",
    "plt.title('Testing results')\n",
    "plt.xlabel(r'time (days)')\n",
    "plt.ylabel(r'y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "nse_train_val = utils.nash_sutcliffe_efficiency(tgt_y_truth_train_val, tgt_y_hat_train_val)\n",
    "rmse_train_val = np.sqrt(mean_squared_error(tgt_y_truth_train_val, tgt_y_hat_train_val))\n",
    "pbias_train_val = utils.pbias(tgt_y_truth_train_val, tgt_y_hat_train_val)\n",
    "kge_train_val = utils.kge(tgt_y_truth_train_val, tgt_y_hat_train_val)\n",
    "print('\\n-- Train/val results')\n",
    "print('NSE = ', nse_train_val)\n",
    "print('RMSE = ', rmse_train_val)\n",
    "print('PBIAS = ', pbias_train_val)\n",
    "print('KGE = ', kge_train_val)\n",
    "\n",
    "nse_test = utils.nash_sutcliffe_efficiency(tgt_y_truth_test, tgt_y_hat_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(tgt_y_truth_test, tgt_y_hat_test))\n",
    "pbias_test = utils.pbias(tgt_y_truth_test, tgt_y_hat_test)\n",
    "kge_test = utils.kge(tgt_y_truth_test, tgt_y_hat_test)\n",
    "print('\\n-- Testing results')\n",
    "print('NSE = ', nse_test)\n",
    "print('RMSE = ', rmse_test)\n",
    "print('PBIAS = ', pbias_test)\n",
    "print('KGE = ', kge_test)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
