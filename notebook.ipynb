{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer for time series prediction\n",
    "These are the inputs and outputs to the model.\n",
    "- Inputs: SWIT + PET.\n",
    "- Output: Streamflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import utils\n",
    "import dataset as ds\n",
    "import transformer as tst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "test_size = 0.2\n",
    "batch_size = 128\n",
    "src_variables = ['X']\n",
    "tgt_variables = ['y']\n",
    "input_variables = src_variables + tgt_variables\n",
    "timestamp_col_name = \"time\"\n",
    "\n",
    "# Only use data from this date and onwards\n",
    "cutoff_date = datetime.datetime(1980, 1, 1) \n",
    "\n",
    "d_model = 256\n",
    "n_heads = 4\n",
    "n_decoder_layers = 1\n",
    "n_encoder_layers = 1\n",
    "encoder_sequence_len = 1461 # length of input given to encoder used to create the pre-summarized windows (4 years of data) 1461\n",
    "crushed_encoder_sequence_len = 53 # Encoder sequence length afther summarizing the data when defining the dataset 53\n",
    "decoder_sequence_len = 1 # length of input given to decoder\n",
    "output_sequence_length = 1 # target sequence length. If hourly data and length = 48, you predict 2 days ahead\n",
    "window_size = encoder_sequence_len + output_sequence_length # used to slice data into sub-sequences\n",
    "step_size = 1 # Step size, i.e. how many time steps does the moving window move at each step\n",
    "in_features_encoder_linear_layer = 128\n",
    "in_features_decoder_linear_layer = 128\n",
    "max_sequence_len = encoder_sequence_len\n",
    "batch_first = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "device = ('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "data = utils.read_data(timestamp_col_name=timestamp_col_name)\n",
    "\n",
    "# Extract train and test data\n",
    "training_data = data[:-(round(len(data)*test_size))]\n",
    "testing_data = data[(round(len(data)*(1-test_size))):]\n",
    "\n",
    "# Normalize the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit scaler on the training set\n",
    "scaler.fit(training_data.iloc[:, 1:])\n",
    "\n",
    "training_data.iloc[:, 1:] = scaler.transform(training_data.iloc[:, 1:])\n",
    "testing_data.iloc[:, 1:] = scaler.transform(testing_data.iloc[:, 1:])\n",
    "\n",
    "# Make list of (start_idx, end_idx) pairs that are used to slice the time series sequence into chuncks\n",
    "training_indices = utils.get_indices(data=training_data, window_size=window_size, step_size=step_size)\n",
    "testing_indices = utils.get_indices(data=testing_data, window_size=window_size, step_size=step_size)\n",
    "\n",
    "# Make instance of the custom dataset class\n",
    "training_data = ds.TransformerDataset(data=torch.tensor(training_data[input_variables].values).float(),\n",
    "                                    indices=training_indices, encoder_sequence_len=encoder_sequence_len, \n",
    "                                    decoder_sequence_len=decoder_sequence_len, tgt_sequence_len=output_sequence_length)\n",
    "testing_data = ds.TransformerDataset(data=torch.tensor(testing_data[input_variables].values).float(),\n",
    "                                    indices=testing_indices, encoder_sequence_len=encoder_sequence_len, \n",
    "                                    decoder_sequence_len=decoder_sequence_len, tgt_sequence_len=output_sequence_length)\n",
    "\n",
    "# Define data for inference\n",
    "inference_data = training_data + testing_data\n",
    "\n",
    "# Set up dataloaders\n",
    "training_data = DataLoader(training_data, batch_size, shuffle=True)\n",
    "testing_data = DataLoader(testing_data, batch_size, shuffle=True)\n",
    "inference_data = DataLoader(inference_data, batch_size=1)\n",
    "\n",
    "# Update the encoder sequence length to its crushed version\n",
    "encoder_sequence_len = crushed_encoder_sequence_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the transformer model and send it to device\n",
    "model = tst.TimeSeriesTransformer(input_size=len(src_variables), decoder_sequence_len=decoder_sequence_len, \n",
    "                                batch_first=batch_first, d_model=d_model, n_encoder_layers=n_encoder_layers, \n",
    "                                n_decoder_layers=n_decoder_layers, n_heads=n_heads, dropout_encoder=0.2, \n",
    "                                dropout_decoder=0.2, dropout_pos_encoder=0.1, dim_feedforward_encoder=in_features_encoder_linear_layer, \n",
    "                                dim_feedforward_decoder=in_features_decoder_linear_layer, num_predicted_features=len(tgt_variables)).to(device)\n",
    "\n",
    "# Make src mask for the decoder with size\n",
    "# [batch_size*n_heads, output_sequence_length, encoder_sequence_len]\n",
    "src_mask = utils.masker(dim1=output_sequence_length, dim2=encoder_sequence_len).to(device)\n",
    "\n",
    "# Make tgt mask for decoder with size\n",
    "# [batch_size*n_heads, output_sequence_length, output_sequence_length]\n",
    "tgt_mask = utils.masker(dim1=output_sequence_length, dim2=output_sequence_length).to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define train and test steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define the training step\n",
    "def train(dataloader, model, loss_function, optimizer, device, df_training, epoch):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    training_loss = [] # For plotting purposes\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        src, tgt, tgt_y = batch\n",
    "        src, tgt, tgt_y = src.to(device), tgt.to(device), tgt.to(device)\n",
    "\n",
    "        # Zero out gradients for every batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute prediction error\n",
    "        pred = model(src=src, tgt=tgt, src_mask=src_mask, tgt_mask=tgt_mask).to(device)\n",
    "        loss = loss_function(pred, tgt_y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Save results for plotting\n",
    "        training_loss.append(loss.item())\n",
    "        epoch_train_loss = np.mean(training_loss)\n",
    "        df_training.loc[epoch] = [epoch, epoch_train_loss]\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print('Current batch', i)\n",
    "            loss, current = loss.item(), (i + 1) * len(src)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "# Define testing step\n",
    "def test(dataloader, model, loss_function, device, df_testing, epoch):\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    testing_loss = [] # For plotting purposes\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            src, tgt, tgt_y = batch\n",
    "            src, tgt, tgt_y = src.to(device), tgt.to(device), tgt_y.to(device)\n",
    "            \n",
    "            pred = model(src=src, tgt=tgt, src_mask=src_mask, tgt_mask=tgt_mask).to(device)\n",
    "            loss = loss_function(pred, tgt_y.unsqueeze(2))\n",
    "            \n",
    "            # Save results for plotting\n",
    "            testing_loss.append(loss.item())\n",
    "            epoch_test_loss = np.mean(testing_loss)\n",
    "            df_testing.loc[epoch] = [epoch, epoch_test_loss]\n",
    "    \n",
    "    loss /= num_batches\n",
    "    print(f\"Avg test loss: {loss:>8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Update model in the training process and test it\n",
    "start_time = time.time()\n",
    "epochs = 10\n",
    "df_training = pd.DataFrame(columns=('epoch', 'loss_train'))\n",
    "df_testing = pd.DataFrame(columns=('epoch', 'loss_test'))\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(training_data, model, loss_function, optimizer, device, df_training, epoch=t)\n",
    "    test(testing_data, model, loss_function, device, df_testing, epoch=t)\n",
    "print(\"Done! ---Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Inference\n",
    "# Get ground truth\n",
    "tgt_y_truth = torch.zeros(len(inference_data))\n",
    "for i, (src, tgt, tgt_y) in enumerate(inference_data):\n",
    "    tgt_y_truth[i] = tgt_y\n",
    "\n",
    "# Define tensor to store the predictions\n",
    "tgt_y_hat = torch.zeros((len(inference_data)), device=device)\n",
    "\n",
    "# Perform inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, sample in enumerate(inference_data):\n",
    "        src, tgt, tgt_y = sample\n",
    "        src, tgt, tgt_y = src.to(device), tgt.to(device), tgt_y.to(device)\n",
    "\n",
    "        pred = model(src=src, tgt=tgt, src_mask=src_mask, tgt_mask=tgt_mask).to(device)\n",
    "        # print(pred, tgt_y)\n",
    "        tgt_y_hat[i] = pred\n",
    "print(tgt_y_hat)\n",
    "# Pass target_y_hat to cpu for plotting purposes\n",
    "tgt_y_hat = tgt_y_hat.cpu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "plt.figure(1);plt.clf()\n",
    "plt.plot(df_training['epoch'], df_training['loss_train'], '-o', label='loss train')\n",
    "plt.plot(df_training['epoch'], df_testing['loss_test'], '-o', label='loss test')\n",
    "plt.xlabel(r'epoch')\n",
    "plt.ylabel(r'loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot inference\n",
    "plt.figure(2);plt.clf()\n",
    "plt.plot(tgt_y_truth, label='observed')\n",
    "plt.plot(tgt_y_hat, label='predicted')\n",
    "plt.xlabel(r'time (days)')\n",
    "plt.ylabel(r'y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "tgt_y_truth_train, tgt_y_truth_test = tgt_y_truth[:-(round(len(tgt_y_truth)*test_size))], tgt_y_truth[(round(len(tgt_y_truth)*(1-test_size))):]\n",
    "tgt_y_hat_train, tgt_y_hat_test = tgt_y_hat[:-(round(len(tgt_y_truth)*test_size))], tgt_y_hat[(round(len(tgt_y_truth)*(1-test_size))):]\n",
    "\n",
    "err = tgt_y_truth_train - tgt_y_hat_train\n",
    "nse = 1 - sum(np.power(err, 2)) / sum(np.power(tgt_y_truth_train - sum(tgt_y_truth_train) / (1-test_size), 2))\n",
    "mse = mean_squared_error(tgt_y_truth_train, tgt_y_hat_train)\n",
    "print('-- training result ')\n",
    "print('NSE = ', nse)\n",
    "print('MSE = ', mse)\n",
    "\n",
    "err = tgt_y_truth_test - tgt_y_hat_test\n",
    "nse = 1 - sum(np.power(err, 2)) / sum(np.power(tgt_y_truth_test - sum(tgt_y_truth_test) / (test_size), 2))\n",
    "mse = mean_squared_error(tgt_y_truth_test, tgt_y_hat_test)\n",
    "print('\\n-- test result ')\n",
    "print('NSE = ', nse)\n",
    "print('MSE = ', mse)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
