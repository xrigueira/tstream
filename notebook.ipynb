{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer for time series modeling\n",
    "These are the inputs and outputs to the model.\n",
    "- Inputs: SWIT + PET.\n",
    "- Output: Streamflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import utils\n",
    "import dataset as ds\n",
    "import transformer as tst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define needed functions: train, test, infer, nse, and parameters printer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define the training step\n",
    "def train(dataloader, model, src_mask, tgt_mask, loss_function, optimizer, device, df_training, epoch):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    training_loss = [] # For plotting purposes\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        src, tgt, tgt_y = batch\n",
    "        src, tgt, tgt_y = src.to(device), tgt.to(device), tgt.to(device)\n",
    "\n",
    "        # Zero out gradients for every batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute prediction error\n",
    "        pred = model(src=src, tgt=tgt, src_mask=src_mask, tgt_mask=tgt_mask).to(device)\n",
    "        loss = loss_function(pred, tgt_y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Save results for plotting\n",
    "        training_loss.append(loss.item())\n",
    "        epoch_train_loss = np.mean(training_loss)\n",
    "        df_training.loc[epoch] = [epoch, epoch_train_loss]\n",
    "        \n",
    "        # if i % 20 == 0:\n",
    "        #     print('Current batch', i)\n",
    "        #     loss, current = loss.item(), (i + 1) * len(src)\n",
    "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "# Define testing step\n",
    "def test(dataloader, model, src_mask, tgt_mask, loss_function, device, df_testing, epoch):\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    testing_loss = [] # For plotting purposes\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            src, tgt, tgt_y = batch\n",
    "            src, tgt, tgt_y = src.to(device), tgt.to(device), tgt_y.to(device)\n",
    "            \n",
    "            pred = model(src=src, tgt=tgt, src_mask=src_mask, tgt_mask=tgt_mask).to(device)\n",
    "            loss = loss_function(pred, tgt_y.unsqueeze(2))\n",
    "            \n",
    "            # Save results for plotting\n",
    "            testing_loss.append(loss.item())\n",
    "            epoch_test_loss = np.mean(testing_loss)\n",
    "            df_testing.loc[epoch] = [epoch, epoch_test_loss]\n",
    "    \n",
    "    loss /= num_batches\n",
    "    # print(f\"Avg test loss: {loss:>8f}\")\n",
    "\n",
    "# Define inference step\n",
    "def inference(inference_data, model, src_mask, tgt_mask, device, test_size):\n",
    "    # Get ground truth\n",
    "    tgt_y_truth = torch.zeros(len(inference_data))\n",
    "    for i, (src, tgt, tgt_y) in enumerate(inference_data):\n",
    "        tgt_y_truth[i] = tgt_y\n",
    "\n",
    "    # Define tensor to store the predictions\n",
    "    tgt_y_hat = torch.zeros((len(inference_data)), device=device)\n",
    "\n",
    "    # Perform inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, sample in enumerate(inference_data):\n",
    "            src, tgt, tgt_y = sample\n",
    "            src, tgt, tgt_y = src.to(device), tgt.to(device), tgt_y.to(device)\n",
    "\n",
    "            pred = model(src=src, tgt=tgt, src_mask=src_mask, tgt_mask=tgt_mask).to(device)\n",
    "            # print(pred, tgt_y)\n",
    "            tgt_y_hat[i] = pred\n",
    "\n",
    "    # Pass target_y_hat to cpu for plotting purposes\n",
    "    tgt_y_hat = tgt_y_hat.cpu()\n",
    "\n",
    "    tgt_y_truth_train, tgt_y_truth_test = tgt_y_truth[:-(round(len(tgt_y_truth)*test_size))].numpy(), tgt_y_truth[(round(len(tgt_y_truth)*(1-test_size))):].numpy()\n",
    "    tgt_y_hat_train, tgt_y_hat_test = tgt_y_hat[:-(round(len(tgt_y_truth)*test_size))].numpy(), tgt_y_hat[(round(len(tgt_y_truth)*(1-test_size))):].numpy()\n",
    "    # np.save('tgt_y_truth.npy', tgt_y_truth, allow_pickle=False, fix_imports=False)\n",
    "    # np.save('tgt_y_hat.npy', tgt_y_hat, allow_pickle=False, fix_imports=False)\n",
    "    \n",
    "    return tgt_y_truth, tgt_y_truth_train, tgt_y_truth_test, tgt_y_hat, tgt_y_hat_train, tgt_y_hat_test\n",
    "\n",
    "# Define function to get and format the number of parameters\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    \n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params += params\n",
    "    \n",
    "    print(table)\n",
    "    print(f\"Total trainable parameters: {total_params}\")\n",
    "    \n",
    "    return total_params\n",
    "\n",
    "# Define Nash-Sutcliffe efficiency\n",
    "def nash_sutcliffe_efficiency(observed, modeled):\n",
    "    mean_observed = np.mean(observed)\n",
    "    numerator = np.sum((observed - modeled)**2)\n",
    "    denominator = np.sum((observed - mean_observed)**2)\n",
    "    \n",
    "    nse = 1 - (numerator / denominator)\n",
    "    \n",
    "    return nse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Hyperparams\n",
    "test_size = 0.2\n",
    "batch_size = 128\n",
    "src_variables = ['X']\n",
    "tgt_variables = ['y']\n",
    "input_variables = src_variables + tgt_variables\n",
    "timestamp_col_name = \"time\"\n",
    "\n",
    "# Only use data from this date and onwards\n",
    "cutoff_date = datetime.datetime(1980, 1, 1) \n",
    "\n",
    "d_model = 32\n",
    "n_heads = 2\n",
    "n_decoder_layers = 1\n",
    "n_encoder_layers = 1\n",
    "encoder_sequence_len = 1461 # length of input given to encoder used to create the pre-summarized windows (4 years of data) 1461\n",
    "crushed_encoder_sequence_len = 53 # Encoder sequence length afther summarizing the data when defining the dataset 53\n",
    "decoder_sequence_len = 1 # length of input given to decoder\n",
    "output_sequence_length = 1 # target sequence length. If hourly data and length = 48, you predict 2 days ahead\n",
    "window_size = encoder_sequence_len + output_sequence_length # used to slice data into sub-sequences\n",
    "step_size = 1 # Step size, i.e. how many time steps does the moving window move at each step\n",
    "in_features_encoder_linear_layer = 32\n",
    "in_features_decoder_linear_layer = 32\n",
    "max_sequence_len = encoder_sequence_len\n",
    "batch_first = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "device = ('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "data = utils.read_data(timestamp_col_name=timestamp_col_name)\n",
    "\n",
    "# Extract train and test data\n",
    "training_data = data[:-(round(len(data)*test_size))]\n",
    "testing_data = data[(round(len(data)*(1-test_size))):]\n",
    "\n",
    "# Normalize the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit scaler on the training set\n",
    "scaler.fit(training_data.iloc[:, 1:])\n",
    "\n",
    "training_data.iloc[:, 1:] = scaler.transform(training_data.iloc[:, 1:])\n",
    "testing_data.iloc[:, 1:] = scaler.transform(testing_data.iloc[:, 1:])\n",
    "\n",
    "# Make list of (start_idx, end_idx) pairs that are used to slice the time series sequence into chuncks\n",
    "training_indices = utils.get_indices(data=training_data, window_size=window_size, step_size=step_size)\n",
    "testing_indices = utils.get_indices(data=testing_data, window_size=window_size, step_size=step_size)\n",
    "\n",
    "# Make instance of the custom dataset class\n",
    "training_data = ds.TransformerDataset(data=torch.tensor(training_data[input_variables].values).float(),\n",
    "                                    indices=training_indices, encoder_sequence_len=encoder_sequence_len, \n",
    "                                    decoder_sequence_len=decoder_sequence_len, tgt_sequence_len=output_sequence_length)\n",
    "testing_data = ds.TransformerDataset(data=torch.tensor(testing_data[input_variables].values).float(),\n",
    "                                    indices=testing_indices, encoder_sequence_len=encoder_sequence_len, \n",
    "                                    decoder_sequence_len=decoder_sequence_len, tgt_sequence_len=output_sequence_length)\n",
    "\n",
    "# Define data for inference\n",
    "inference_data = training_data + testing_data\n",
    "\n",
    "# Set up dataloaders\n",
    "training_data = DataLoader(training_data, batch_size, shuffle=True)\n",
    "testing_data = DataLoader(testing_data, batch_size, shuffle=True)\n",
    "inference_data = DataLoader(inference_data, batch_size=1)\n",
    "\n",
    "# Update the encoder sequence length to its crushed version\n",
    "encoder_sequence_len = crushed_encoder_sequence_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the transformer model and send it to device\n",
    "model = tst.TimeSeriesTransformer(input_size=len(src_variables), decoder_sequence_len=decoder_sequence_len, \n",
    "                                batch_first=batch_first, d_model=d_model, n_encoder_layers=n_encoder_layers, \n",
    "                                n_decoder_layers=n_decoder_layers, n_heads=n_heads, dropout_encoder=0.2, \n",
    "                                dropout_decoder=0.2, dropout_pos_encoder=0.1, dim_feedforward_encoder=in_features_encoder_linear_layer, \n",
    "                                dim_feedforward_decoder=in_features_decoder_linear_layer, num_predicted_features=len(tgt_variables)).to(device)\n",
    "\n",
    "# Make src mask for the decoder with size\n",
    "# [batch_size*n_heads, output_sequence_length, encoder_sequence_len]\n",
    "src_mask = utils.masker(dim1=output_sequence_length, dim2=encoder_sequence_len).to(device)\n",
    "\n",
    "# Make tgt mask for decoder with size\n",
    "# [batch_size*n_heads, output_sequence_length, output_sequence_length]\n",
    "tgt_mask = utils.masker(dim1=output_sequence_length, dim2=output_sequence_length).to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Update model in the training process and test it\n",
    "epochs = 250\n",
    "start_time = time.time()\n",
    "df_training = pd.DataFrame(columns=('epoch', 'loss_train'))\n",
    "df_testing = pd.DataFrame(columns=('epoch', 'loss_test'))\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(training_data, model, src_mask, tgt_mask, loss_function, optimizer, device, df_training, epoch=t)\n",
    "    test(testing_data, model, src_mask, tgt_mask, loss_function, device, df_testing, epoch=t)\n",
    "print(\"Done! ---Execution time: %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Save the model\n",
    "torch.save(model, \"models/model.pth\")\n",
    "print(\"Saved PyTorch entire model to models/model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Inference\n",
    "tgt_y_truth, tgt_y_truth_train, tgt_y_truth_test, tgt_y_hat, tgt_y_hat_train, tgt_y_hat_test = inference(inference_data, model, src_mask, tgt_mask, device, test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "plt.figure(1);plt.clf()\n",
    "plt.plot(df_training['epoch'], df_training['loss_train'], '-o', label='loss train')\n",
    "plt.plot(df_training['epoch'], df_testing['loss_test'], '-o', label='loss test')\n",
    "plt.yscale('log')\n",
    "plt.xlabel(r'epoch')\n",
    "plt.ylabel(r'loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot inference\n",
    "plt.figure(2);plt.clf()\n",
    "plt.plot(tgt_y_truth, label='observed')\n",
    "plt.plot(range(len(tgt_y_hat_train)), tgt_y_hat_train, label='predicted train')\n",
    "plt.plot(range(len(tgt_y_hat_train), len(tgt_y_hat)), tgt_y_hat_test, color='lightskyblue', label='predicted test')\n",
    "plt.xlabel(r'time (days)')\n",
    "plt.ylabel(r'y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "nse_train = nash_sutcliffe_efficiency(tgt_y_truth_train, tgt_y_hat_train)\n",
    "mse_train = mean_squared_error(tgt_y_truth_train, tgt_y_hat_train)\n",
    "print('-- training result ')\n",
    "print('NSE = ', nse_train)\n",
    "print('MSE = ', mse_train)\n",
    "\n",
    "nse_test = nash_sutcliffe_efficiency(tgt_y_truth_test, tgt_y_hat_test)\n",
    "mse_test = mean_squared_error(tgt_y_truth_test, tgt_y_hat_test)\n",
    "print('\\n-- test result ')\n",
    "print('NSE = ', nse_test)\n",
    "print('MSE = ', mse_test)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
