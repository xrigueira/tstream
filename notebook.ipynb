{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer for time series prediction\n",
    "These are the inputs and outputs to the model.\n",
    "- Inputs: SWIT + PET.\n",
    "- Output: Streamflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import utils\n",
    "import dataset as ds\n",
    "import transformer as tst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "test_size = 0.1\n",
    "batch_size = 128\n",
    "src_variables = ['X']\n",
    "tgt_variables = ['y']\n",
    "input_variables = src_variables + tgt_variables\n",
    "timestamp_col_name = \"time\"\n",
    "\n",
    "d_model = 128\n",
    "n_heads = 2\n",
    "n_decoder_layers = 1\n",
    "n_encoder_layers = 1\n",
    "encoder_sequence_len = 1461 # length of input given to encoder used to create the pre-summarized windows (4 years of data)\n",
    "crushed_encoder_sequence_len = 53 # Encoder sequence length afther summarizing the data when defining the dataset\n",
    "decoder_sequence_len = 1 # length of input given to decoder\n",
    "output_sequence_length = 1 # target sequence length. If hourly data and length = 48, you predict 2 days ahead\n",
    "window_size = encoder_sequence_len + output_sequence_length # used to slice data into sub-sequences\n",
    "step_size = 1 # Step size, i.e. how many time steps does the moving window move at each step\n",
    "in_features_encoder_linear_layer = 32\n",
    "in_features_decoder_linear_layer = 32\n",
    "max_sequence_len = encoder_sequence_len\n",
    "batch_first = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "device = ('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "data = utils.read_data(timestamp_col_name=timestamp_col_name)\n",
    "\n",
    "# Extract train and test data\n",
    "training_data = data[:-(round(len(data)*test_size))]\n",
    "testing_data = data[(round(len(data)*(1-test_size))):]\n",
    "\n",
    "# Make list of (start_idx, end_idx) pairs that are used to slice the time series sequence into chuncks\n",
    "training_indices = utils.get_indices(data=training_data, window_size=window_size, step_size=step_size)\n",
    "testing_indices = utils.get_indices(data=testing_data, window_size=window_size, step_size=step_size)\n",
    "\n",
    "# Make instance of the custom dataset class\n",
    "training_data = ds.TransformerDataset(data=torch.tensor(training_data[input_variables].values).float(),\n",
    "                                    indices=training_indices, encoder_sequence_len=encoder_sequence_len, \n",
    "                                    decoder_sequence_len=decoder_sequence_len, tgt_sequence_len=output_sequence_length)\n",
    "testing_data = ds.TransformerDataset(data=torch.tensor(testing_data[input_variables].values).float(),\n",
    "                                    indices=testing_indices, encoder_sequence_len=encoder_sequence_len, \n",
    "                                    decoder_sequence_len=decoder_sequence_len, tgt_sequence_len=output_sequence_length)\n",
    "\n",
    "# Define data for inference\n",
    "inference_data = training_data + testing_data\n",
    "\n",
    "# Make dataloaders\n",
    "training_data = DataLoader(training_data, batch_size)\n",
    "testing_data = DataLoader(testing_data, batch_size)\n",
    "\n",
    "# Update the encoder sequence length to its crushed version\n",
    "encoder_sequence_len = crushed_encoder_sequence_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the transformer model and send it to device\n",
    "model = tst.TimeSeriesTransformer(input_size=len(src_variables), decoder_sequence_len=decoder_sequence_len, \n",
    "                                batch_first=batch_first, num_predicted_features=len(tgt_variables)).to(device)\n",
    "\n",
    "# Make src mask for the decoder with size\n",
    "# [batch_size*n_heads, output_sequence_length, encoder_sequence_len]\n",
    "src_mask = utils.masker(dim1=output_sequence_length, dim2=encoder_sequence_len).to(device)\n",
    "\n",
    "# Make tgt mask for decoder with size\n",
    "# [batch_size*n_heads, output_sequence_length, output_sequence_length]\n",
    "tgt_mask = utils.masker(dim1=output_sequence_length, dim2=output_sequence_length).to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define train and test steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define the training step\n",
    "def train(dataloader, model, loss_function, optimizer, device, df_training, epoch):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    training_loss = [] # For plotting purposes\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        src, tgt, tgt_y = batch\n",
    "        src, tgt, tgt_y = src.to(device), tgt.to(device), tgt.to(device)\n",
    "        \n",
    "        # Compute prediction error\n",
    "        pred = model(src=src, tgt=tgt, src_mask=src_mask, tgt_mask=tgt_mask).to(device)\n",
    "        loss = loss_function(pred, tgt_y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Save results for plotting\n",
    "        training_loss.append(loss.item())\n",
    "        epoch_train_loss = np.mean(training_loss)\n",
    "        df_training.loc[epoch] = [epoch, epoch_train_loss]\n",
    "        \n",
    "        print('Current batch', i)\n",
    "        if i % 1== 0:\n",
    "            loss, current = loss.item(), (i + 1) * len(src)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "# Define testing step\n",
    "def test(dataloader, model, loss_function, device, df_testing, epoch):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    testing_loss = [] # For plotting purposes\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            src, tgt, tgt_y = batch\n",
    "            src, tgt, tgt_y = src.to(device), tgt.to(device), tgt_y.to(device)\n",
    "            \n",
    "            pred = model(src=src, tgt=tgt, src_mask=src_mask, tgt_mask=tgt_mask).to(device)\n",
    "            test_loss += loss_function(pred, tgt_y.unsqueeze(2))\n",
    "            \n",
    "            # Save results for plotting\n",
    "            testing_loss.append(test_loss.item())\n",
    "            epoch_test_loss = np.mean(testing_loss)\n",
    "            df_testing.loc[epoch] = [epoch, epoch_test_loss]\n",
    "    \n",
    "    test_loss /+ num_batches\n",
    "    print(f\"Avg loss: {test_loss:>8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Update model in the training process and test it\n",
    "epochs = 1\n",
    "df_training = pd.DataFrame(columns=('epoch', 'loss_train'))\n",
    "df_testing = pd.DataFrame(columns=('epoch', 'loss_test'))\n",
    "for t in range (epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(training_data, model, loss_function, optimizer, device, df_training, epoch=t)\n",
    "    test(testing_data, model, loss_function, device, df_testing, epoch=t)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Inference\n",
    "# Get ground truth\n",
    "tgt_y_truth = torch.zeros(len(inference_data))\n",
    "for i, (src, tgt, tgt_y) in enumerate(inference_data):\n",
    "    tgt_y_truth[i] = tgt_y\n",
    "\n",
    "tgt_y_hat = torch.zeros((len(inference_data)), device=device)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    for i, sample in enumerate(inference_data):\n",
    "        src, tgt, tgt_y = sample\n",
    "        src, tgt, tgt_y = src.unsqueeze(0), tgt.unsqueeze(0), tgt_y.unsqueeze(0)\n",
    "        \n",
    "        src, tgt, tgt_y = src.to(device), tgt.to(device), tgt_y.to(device)\n",
    "\n",
    "        pred = model(src=src, tgt=tgt, src_mask=src_mask, tgt_mask=tgt_mask).to(device)\n",
    "        tgt_y_hat[i] = pred\n",
    "\n",
    "# Pass target_y_hat to cpu for plotting purposes\n",
    "tgt_y_hat = tgt_y_hat.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "plt.figure(1);plt.clf()\n",
    "plt.plot(df_training['epoch'], df_training['loss_train'], '-o', label='loss train')\n",
    "plt.plot(df_training['epoch'], df_testing['loss_test'], '-o', label='loss test')\n",
    "plt.xlabel(r'epoch')\n",
    "plt.ylabel(r'loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot inference\n",
    "plt.figure(2);plt.clf()\n",
    "plt.plot(tgt_y_truth, label='observed')\n",
    "plt.plot(tgt_y_hat, label='predicted')\n",
    "plt.xlabel(r'time (days)')\n",
    "plt.ylabel(r'y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
